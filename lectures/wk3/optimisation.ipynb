{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('py39': conda)"
  },
  "interpreter": {
   "hash": "88b11310c3bf316e37a708cb8d0dbf79bba9a2d571f5024d0d39a9df939cbe1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# week 3: optimisation\n",
    "\n",
    "## local search\n",
    "\n",
    "search algorithms that maintain a single node and searches by moving to a neighbouring node\n",
    "\n",
    "**state-space landscape**: generating a landscape to visualise all the configuration costs to find a global maxima or minima\n",
    "\n",
    "**hill climbing**: to find the maxima, start at a state and find neighbours and their cost function, moving to the next heighest\n",
    "- this doesn't result in the most optimal solution as it will only find its local minima, not the global minima\n",
    "\n",
    "### other types\n",
    "- Steepest-ascent: choose the highest-valued neighbor. This is the standard variation that we discussed above.\n",
    "- Stochastic: choose randomly from higher-valued neighbors. Doing this, we choose to go to any direction that improves over our value. This makes sense if, for example, the highest-valued neighbor leads to a local maximum while another neighbor leads to a global maximum.\n",
    "- First-choice: choose the first higher-valued neighbor.\n",
    "- Random-restart: conduct hill climbing multiple times. Each time, start from a random state. Compare the maxima from every trial, and choose the highest amongst those.\n",
    "- Local Beam Search: chooses the k highest-valued neighbors. This is unlike most local search algorithms in that it uses multiple nodes for the search, and not just one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## simulated annealing\n",
    "\n",
    "we can't always make the best move to find global minima/maxima, so this simulates the annealing process\n",
    "- start with a high temperature, more likely to accept neighbours that are worse than the current state\n",
    "- over time the temperature cools, and worse choices are less likely\n",
    "\n",
    "## linear programming\n",
    "- optimising a linear mathematical function\n",
    "- minimise the cost (every $x_n$ has a cost $c_n$)\n",
    "- problems have constraints; the sum of the variables that is less than or equal to a value ($a_1x_1+a_2x_2+...+a_nx_n\\le b$)\n",
    "- each coefficient of x has some resources associated, and $b$ is how much resources we can dedicate to the problems\n",
    "### solutions\n",
    "- simplex\n",
    "- interior-point"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### example\n",
    "\n",
    "- There are two machines, $x_1$ and $x_2$\n",
    "- $x_1$ costs $50/hr to run\n",
    "- $x_2$ costs $80/hr to run\n",
    "- goal: minimise cost of $x_1+x_2$\n",
    "- $x_1$ requires 5 units of labour an hour\n",
    "- $x_2$ requires 2 units of labour an hour\n",
    "- we have 20 units of labour to spend: $5x_1+2x_2\\le 20$\n",
    "- $x_1$ produces 10 units of output an hour\n",
    "- $x_2$ produces 12 units of output an hour\n",
    "- we need 90 units of output: $(-10x_1)+(-12x_2)\\le -90$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X1: 1.5 hours\nX2: 6.25 hours\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "# Objective Function: 50x_1 + 80x_2\n",
    "# Constraint 1: 5x_1 + 2x_2 <= 20\n",
    "# Constraint 2: -10x_1 + -12x_2 <= -90\n",
    "\n",
    "result = scipy.optimize.linprog(\n",
    "    [50, 80],  # Cost function: 50x_1 + 80x_2\n",
    "    A_ub=[[5, 2], [-10, -12]],  # Coefficients for inequalities (ub = upper bound)\n",
    "    b_ub=[20, -90],  # Constraints for inequalities: 20 and -90\n",
    ")\n",
    "\n",
    "if result.success:\n",
    "    print(f\"X1: {round(result.x[0], 2)} hours\")\n",
    "    print(f\"X2: {round(result.x[1], 2)} hours\")\n",
    "else:\n",
    "    print(\"No solution\")"
   ]
  },
  {
   "source": [
    "## constraint graphs\n",
    "\n",
    "- **Hard Constraint**: a constraint that must be satisfied in a correct solution.\n",
    "- **Soft Constraint**: a constraint that expresses which solution is preferred over others.\n",
    "- **Unary Constraint**: a constraint that involves only one variable. In our example, a unary constraint would be saying that course A can’t have an exam on Monday {A ≠ Monday}.\n",
    "- **Binary Constraint**: a constraint that involves two variables. This is the type of constraint that we used in the example above, saying that some two courses can’t have the same value {A ≠ B}.\n",
    "\n",
    "### node consistent\n",
    "\n",
    "- When all the values in a variable's domain satisfy the variable's unary constraints\n",
    "\n",
    "### arc consistent\n",
    "\n",
    "- when all the values in a variable's domain satisfy the variable's binary constraints (arc = edge)\n",
    "- to make X arc consistent with Y, remove elements in X's domain until every choice for X has a possible choice for Y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}